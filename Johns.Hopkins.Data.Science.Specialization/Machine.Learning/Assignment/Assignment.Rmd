---
title: "Assignment-Machine.Learning"
author: "Douglas Thoms"
date: "September 19, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Parameters

```{r}
sessionInfo()
```

```{r}
library(dplyr)
library(ggplot2)
library(caret)
library(parallel)
library(doParallel)
library(earth)
```

## Project Parameters

<2000 words
=< 5 figures
-classe distribution
-out of error rate
-accuracy compared
in htlm

how you built your model
how you used cross validation
what you think the expected out of sample error is
and why you made the choices you did




### Sections

## Prediction objective

-This group studied the quality of exercise, would they be able to classify the
quality of exercise and errors using physical sensors of movement

-The paramaters of this classification exercise are that any variables in training can
be used to correctly classify the classe variable

-The classe variable represents the type of error with "A" representing a correct
dumbell curve and "B","C","D" and "E" representing type errors in technique.

##Inputs  

* The data has already been partitioned in a training and testing sets.  
* No codebook has been provided but the research paper has been providing summarizing
the methodology of the study and the context of the data  
  


```{r echo=FALSE,eval=TRUE}
if(!file.exists("training.csv")){
        download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                      destfile = "C:/Users/dthoms/Documents/Training/Coursera/Johns.Hopkins.Data.Science.Specialization/Machine.Learning/Assignment/training.csv")
}

#read sources and put in NA in blank
training = read.csv("C:/Users/dthoms/Documents/Training/Coursera/Johns.Hopkins.Data.Science.Specialization/Machine.Learning/Assignment/training.csv",
                    na.strings=c("","NA"))

if(!file.exists("testing.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
              destfile = "C:/Users/dthoms/Documents/Training/Coursera/Johns.Hopkins.Data.Science.Specialization/Machine.Learning/Assignment/testing.csv")
}

#read sources and put in NA in blank
testing = read.csv("C:/Users/dthoms/Documents/Training/Coursera/Johns.Hopkins.Data.Science.Specialization/Machine.Learning/Assignment/testing.csv",
                   na.strings=c("","NA"))
```

```{r eval=TRUE}
dim(training)
head(select(training,1:10))
dim(testing)
head(select(testing,1:10))
```

* therefore, some individual variables are ambigious in their meaning such as
num_window and new_window - investigations will be need to determine these variables  

## Cleaning Data, Exploratory Analysis

```{r echo=FALSE}
#function to see which columns have all NA
take.NA = function(x){
                 if(sum(is.na(x)) == length(x)){
                        x="TRUE"
                 } else{
                        x="non NA values present"
                 }
        }

#remove all values that are completely NA
testing.non.zero.values <- apply(testing, 2, take.NA)

#remove all NA columns
testing.proc <- testing[,testing.non.zero.values != TRUE]

#create vector to select to remove vectors in training that were removed from testing
training.remove.vectors <- names(testing)[testing.non.zero.values == TRUE]

#remove columns
training.proc.raw <- training[, !colnames(training) %in% training.remove.vectors]

#check for NA, empty spaces in observation
empty.cells <- complete.cases(training.proc.raw)[FALSE]

#remove variables with unlikely relation like training window, etc
training.proc.raw <- select(training.proc.raw, -X, -user_name,-raw_timestamp_part_1,
          -raw_timestamp_part_2, -cvtd_timestamp)

training.proc.num_window <- training.proc.raw[training.proc.raw$new_window == "no", ] 
```

* Two variables were unclear,"new_window" and "num_window".  Since there is no
codebook, the study was used.  According to the study, subjects did one set of 10 repetitions
of each "class" of exercise.  The researchers would record the data using a sliding window
ranging from 0.5 secs to 2.5 secs.  The "new_window" variables that are "yes" have the calculated
variables like minimum, maximum, etc.  This suggests these observations represent the end
of the sliding windows were all the values are calculated.  Regarding "num_window", the num_window values all consistently refer to the same classe variable.  For instance, all "num_window" 11 values correspond to classe "A".  This suggests num_window either represents a set of repetitions
or individual repetitions  
  
* The testing set has columns that are exclusively NA.  On further examination
this is because these columns are only populated when "new_window" equals "yes".  
In the testing dataset there are no observations where "new_window" equals "yes".  Therefore, these columns were removed from both the training and testing sets since they would not be useful as predictors.  
  
* Some variables shared by both testing and training were excluded as perdictors
as they had no obvious relation to classifying the repetitions.  Removing these predictors
improved the performance of the models.  

* The classified variable is discrete and not continuous.  Therefore, a simple
count bar graph was used to see the distribution of types of exercise.  
  
```{r echo = FALSE, eval=TRUE}
#create table of frequency of classe per num_window
#using code below to create table and count number of num_window that
#have only one type of classe e.  All num_window have exclusively one classe
table.classe.training.proc <- table(training.proc.num_window$num_window, training.proc.num_window$classe)
table.classe.training.proc <- data.frame(table.classe.training.proc)
table.classe.training.proc <- table.classe.training.proc[table.classe.training.proc$Freq > 0, ] 
classe.per.num_window <- count(table.classe.training.proc, Var1)
#length of vector is 857, same as all observations
freq.1.classe.num_window <- length(classe.per.num_window[classe.per.num_window$n == 1,]$Var1)

#num_window will be treated as a repetition 

#testing.proc num_window vs new_window
training.proc.yes <- training.proc.raw[training.proc.raw$new_window == "yes", ]

#plotting predictors week 2 - box plot with overlays - factors
#density plots

classe.dist <- aggregate(Freq~Var2, data = table.classe.training.proc, sum)

#distribution see if classe is equally distributed
plot1.obj <- ggplot(data = classe.dist, aes(x = Var2, y = Freq, fill = Var2)) +
        geom_bar(stat = "identity") + 
        labs(title = "Distribution of \'classe\' Variables", x = "", y = "") +
        theme(legend.position = "none")

plot(plot1.obj)
```

## Features


##Evaluation

-why did I choose 5 k-folds - do these provide of out sample example?


```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:



Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
