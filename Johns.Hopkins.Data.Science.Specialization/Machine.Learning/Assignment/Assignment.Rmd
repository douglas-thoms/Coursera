---
title: "Assignment-Machine.Learning"
author: "Douglas Thoms"
date: "September 19, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Parameters

```{r}
sessionInfo()
```

```{r}
library(dplyr)
library(ggplot2)
library(caret)
library(parallel)
library(doParallel)
library(earth)
```

## Project Parameters

<2000 words
=< 5 figures
-classe distribution
-sd vs mean
-out of error rate
-accuracy compared
in htlm

how you built your model
how you used cross validation
what you think the expected out of sample error is
and why you made the choices you did




### Sections

## Prediction objective

-This group studied the quality of exercise, would they be able to classify the
quality of exercise and errors using physical sensors of movement

-The paramaters of this classification exercise are that any variables in training can
be used to correctly classify the classe variable

-The classe variable represents the type of error with "A" representing a correct
dumbell curve and "B","C","D" and "E" representing type errors in technique.

##Inputs  

* The data has already been partitioned in a training and testing sets.  
* No codebook has been provided but the research paper has been providing summarizing
the methodology of the study and the context of the data  
  


```{r echo=FALSE,eval=TRUE}
if(!file.exists("training.csv")){
        download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                      destfile = "C:/Users/dthoms/Documents/Training/Coursera/Johns.Hopkins.Data.Science.Specialization/Machine.Learning/Assignment/training.csv")
}

#read sources and put in NA in blank
training = read.csv("C:/Users/dthoms/Documents/Training/Coursera/Johns.Hopkins.Data.Science.Specialization/Machine.Learning/Assignment/training.csv",
                    na.strings=c("","NA"))

if(!file.exists("testing.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
              destfile = "C:/Users/dthoms/Documents/Training/Coursera/Johns.Hopkins.Data.Science.Specialization/Machine.Learning/Assignment/testing.csv")
}

#read sources and put in NA in blank
testing = read.csv("C:/Users/dthoms/Documents/Training/Coursera/Johns.Hopkins.Data.Science.Specialization/Machine.Learning/Assignment/testing.csv",
                   na.strings=c("","NA"))
```

```{r eval=TRUE}
dim(training)
head(select(training,1:10))
dim(testing)
head(select(testing,1:10))
```

* Some individual variables are ambigious in their meaning such as
num_window and new_window - investigations will be need to determine these variables.  

## Cleaning Data, Exploratory Analysis

```{r echo=FALSE}
#function to see which columns have all NA
take.NA = function(x){
                 if(sum(is.na(x)) == length(x)){
                        x="TRUE"
                 } else{
                        x="non NA values present"
                 }
        }

#remove all values that are completely NA
testing.non.zero.values <- apply(testing, 2, take.NA)

#remove all NA columns
testing.proc <- testing[,testing.non.zero.values != TRUE]

#create vector to select to remove vectors in training that were removed from testing
training.remove.vectors <- names(testing)[testing.non.zero.values == TRUE]

#remove columns
training.proc.raw <- training[, !colnames(training) %in% training.remove.vectors]

#check for NA, empty spaces in observation
empty.cells <- complete.cases(training.proc.raw)[FALSE]

#remove variables with unlikely relation like training window, etc
training.proc.raw <- select(training.proc.raw, -X, -user_name,-raw_timestamp_part_1,
          -raw_timestamp_part_2, -cvtd_timestamp)

training.proc.num_window <- training.proc.raw[training.proc.raw$new_window == "no", ] 
```

* Two variables were unclear, "new_window" and "num_window".  Since there is no
codebook, the study was used as reference.  
  
According to the study, subjects did one set of 10 repetitionsof each "class" of exercise.  The researchers would record the data using a sliding window ranging from 0.5 secs to 2.5 secs.  The "new_window" variables that are "yes" have the calculated variables like minimum, maximum, etc.  

This suggests these observations represent the end of the sliding windows were all the values are calculated.  Regarding "num_window", the num_window values all consistently refer to the same classe variable.  For instance, all "num_window" 11 values correspond to classe "A".  This suggests num_window either represents a set of repetitions or individual repetitions. This was determined using the code below. 

```{r echo = TRUE}
#create table of frequency of classe per num_window
#using code below to create table and count number of num_window that
#have only one type of classe e.  All num_window have exclusively one classe
table.classe.training.proc <- table(training.proc.num_window$num_window, training.proc.num_window$classe)
table.classe.training.proc <- data.frame(table.classe.training.proc)
table.classe.training.proc <- table.classe.training.proc[table.classe.training.proc$Freq > 0, ] 
classe.per.num_window <- count(table.classe.training.proc, Var1)

#length of vector is 857, same as all observations
freq.1.classe.num_window <- length(classe.per.num_window[classe.per.num_window$n == 1,]$Var1)
#num_window will be treated as a repetition
```
  
* The testing set has columns that are exclusively NA.  On further examination this is because these columns are the caculated values and are populated when the "new_window" variable equals "yes" - none of the testing "new_window" variables equals "yes".  Therefore, these columns were removed from both the training and testing sets since they would not be useful as predictors.  
  
* Some variables shared by both testing and training were excluded as perdictors
as they had no obvious relation to classifying the repetitions.  Removing these predictors
improved the performance of the models.  


  


## Features

* The classified variable is discrete and not continuous.  Therefore, a simple
count bar graph was used to see the distribution of types of exercise.  All of the 
potential outcomes have a reasonable amounts of observations.  It looks like there
is a reasonable balance of outcomes and predictors

```{r echo = FALSE, eval=TRUE}
#check distribution of casse

classe.dist <- aggregate(Freq~Var2, data = table.classe.training.proc, sum)
plot1.obj <- ggplot(data = classe.dist, aes(x = Var2, y = Freq, fill = Var2)) +
        geom_bar(stat = "identity") + 
        labs(title = "Distribution of \'classe\' Variables", x = "", y = "") +
        theme(legend.position = "none")
plot(plot1.obj)
```

* The mean and standard deviation were graphed to see if there were any extreme outliers or unusual values.  None were found.
```{r echo = FALSE, eval=TRUE}
ave.mean.test <- select(training.proc.num_window, -num_window, -new_window, -classe)

ave.mean.test <- t(rbind(summarise_each(ave.mean.test, mean), summarise_each(ave.mean.test, sd)))
ave.mean.test <- data.frame(ave.mean.test)
names(ave.mean.test) <- c("mean", "SD")

plot2.obj <- ggplot(data = ave.mean.test, aes(x = mean, y = SD)) +
        geom_jitter() +
        labs(title = "Comparison of mean vs standard distirbution", 
             x = "Mean", y = "Standard Deviation") +
        theme(legend.position = "none")
plot(plot2.obj)

```

The review suggests no need for feature modification and preprocessing.

## Algorithims

* Parallel processing was enabled to speed processing  
  
* K-fold cross validation was included using trainControl function.    


```{r cache = TRUE}
#random forest algorithm
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

fitControl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE)

#fit.rf.obj <- train(classe~., method="rf", data = training.proc.num_window[c(-1,-2)],
#                    trControl = fitControl)

```

```{r cache = TRUE}
#fit.gbm.obj <- train(classe~., method="gbm", data = training.proc.num_window[c(-1,-2)],
#                    trControl = fitControl)
```

```{r cache = TRUE}
#fit.lda.obj <- train(classe~., method="lda", data = training.proc.num_window[c(-1,-2)],
#                     trControl = fitControl)

stopCluster(cluster)
registerDoSEQ()
```

##Evaluation



5 k-folds were chosen since it represents 20% of testing data.

PROVIDE A FIGURE COMPARING OUT OF SAMPLE from different models

THEN PROVIDE CHART ON random forest

