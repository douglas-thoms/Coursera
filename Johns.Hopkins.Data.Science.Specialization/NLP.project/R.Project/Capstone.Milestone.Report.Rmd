---
title: "Capstone Project Milestone Report"
author: "Douglas Thoms"
date: "November 2, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Goals of App and Algorithm
### Algorithm
* Predict a set of words (up to 4 words) using a preceding set of words (up to 4 words)

### App
* The app will take the last 4 words of the sentence and use the algorithm to predict the most likely set of following words  

* The app and data will be small enough to post to the shinyapps.io website document (less than 1GB)

## Feature of App and Algorithm
### Algorithm
* The algorithm will initially use the smart back-off model since it is easy to code, simple and known to be effective  

* May also try the knesey-ney model if appropriate,  

### App
* User will input a sentence into an input field
  
* A warning will be generated by app if the most recent word has no corresponding observed bi-gram in the data, 
  
* App will add the prediction to the end of the input if algorithm creates a prediction, 

## Plan to Create Model and App
1. Download data
2. Sample Data
3. Conduct preliminary Exploratory Data Analysis
4. Research word prediction models and choose potential models
5. Construct Model
6. Quality check model
7. Optimize model to operate on shinyapps.io model
8. Create presentation

## Summary of Basic Dataset Features
* Source of Data - https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip, from SwiftKey  
  
* Collection of English language news, blogs and tweets from internet (foreign words, number, slang and non-words may be included)  
  
* Data is raw so punctuation and unnecessary words will need to be removed  

```{r eval=TRUE, echo=FALSE, message=FALSE}
##----------------------------------------------------------------------------
## Libraries, data and system
##----------------------------------------------------------------------------

session.info.list <- sessionInfo()

library(quanteda)
library(dplyr)
library(ggplot2)
library(lexicon)
library(stringr)

data(profanity_zac_anger)
data(grady_augmented)


#for reproducibility, same randomness
set.seed(3353)


home.directory <- getwd()
data.directory <- paste(home.directory,"data", sep = "/")
training.data.file.path <- paste(data.directory,"Coursera-SwiftKey.zip",                                                          
                           sep = "/")

dir.create(data.directory,showWarnings = FALSE)
```

```{r eval=TRUE, echo=FALSE,cache=TRUE, message=FALSE, warning=FALSE, include=FALSE}
##----------------------------------------------------------------------------
## Functions
##----------------------------------------------------------------------------

#read lines from a text file, sampling them using rbinom
get.lines <- function(df,type.info) {
        con <- file(as.character(df[type.info,2]),'rb')
        x <- 0
        #tmp2 <- as.character()
        
        for(i in 1:df[type.info,1]){
                tmp <- readLines(con, 1, encoding = "UTF-8", skipNul = TRUE)
                
                if(rbinom(1,1,sample.rate) & length(tmp)){
                        x <- x + 1
                        if(x == 1) tmp2 <- tmp else tmp2 <- c(tmp2,tmp)
                        
                }
                
        }
        close(con)
        return(as.character(tmp2))
}

#creates corpus from large matrix, adds URL and source name is docvars
create.corpus <- function(input,text_name,file,URL){
        
        output <- corpus(input, docnames = rep(text_name,length(input)))
        docvars(output, "source") <- as.character(file)
        docvars(output, "URL") <- as.character(URL)
        return(output)
}

#create matrix of statistics
get.stats <- function(dfm){
        return(data.frame(num.features = nfeat(dfm),
                          sparsity = sparsity(dfm),
                          num.docs = ndoc(dfm)))
               
}



##----------------------------------------------------------------------------
## Acquiredata and Clean
##---------------------------------------------------------------------------

#download profanity list

training.data.loc <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"


#download and unzip file if not present
if(!file.exists(training.data.file.path)){
download.file(training.data.loc, training.data.file.path)

training.data.date <- date()


unzip(training.data.file.path, exdir = data.directory)

}

#determine sample - assume normal distribution
# 95% confidence interval
# Sample Size Calculation:
#         Sample Size = (Distribution of 50%) / ((Margin of Error% / Confidence Level Score)Squared)
# Finite Population Correction:
#         True Sample = (Sample Size X Population) / (Sample Size + Population â€“ 1)


#create data frame of 3 different files info
input.info.df <- data.frame(
        num.lines = c(1010274, 899289, 2360149),
        path = c('.//data//final//en_US//en_US.news.txt',
                 './/data//final//en_US//en_US.blogs.txt',
                 './/data//final//en_US//en_US.twitter.txt'),
        names = c('news','blogs','twitter')
)


#determine sample rate to use
news.sample <- (0.5 * (1-0.5))/((.05/2.576)^2)
news.sample.size <- (news.sample * input.info.df[1,1])/(news.sample + input.info.df[1,1] - 1)

blogs.sample <- (0.5 * (1-0.5))/((.05/2.576)^2)
blogs.sample.size <- (blogs.sample * input.info.df[2,1])/(blogs.sample + input.info.df[2,1] - 1)

twitter.sample <- (0.5 * (1-0.5))/((.05/2.576)^2)
twitter.sample.size <- (twitter.sample * input.info.df[3,1])/(twitter.sample + input.info.df[3,1] - 1)

sample.rate = 1000/input.info.df[2,1]*10
#use sample of 2000

#use function to read lines from text file
news<- get.lines(input.info.df,1)
blogs<- get.lines(input.info.df,2)
twitter<- get.lines(input.info.df,3)

#create corpus from matrices

news.corpus <- create.corpus(news,"news.sample","en_US.news.txt",
                             "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip")
blogs.corpus <- create.corpus(blogs,"blogs.sample", "en_US.blogs.txt",
                              "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip")
twitter.corpus <- create.corpus(twitter, "twitter.sampletex", "en_US.twitter.txt",
                                "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip")

#create one corpus
total.corpus <- corpus(news.corpus) + corpus(blogs.corpus) + corpus(twitter.corpus)



##----------------------------------------------------------------------------
## Clean Data/Tokenization/Ngrams
##----------------------------------------------------------------------------


#create dictionary of words to exclude
#add profane from lexicon
dict.profane <- dictionary(list(profanity = profanity_zac_anger))

#create regex expression to exclude
dict.regex <- dictionary(list(at.mark = "[@!#%&()*+./<=>_]",
                        number = "[0-9]-[0-9]"
))

#add names - US and stuff
#See if these others work later
dict.english <- dictionary(list(grady = grady_augmented
                                ))

#use lexicon grady_augmented and profane

#make do with worstemming for now, if need lemitization use later

#create tokens
#remove profane, stopwords and non-words
total.tokens <- total.corpus %>%
                tokens(remove_punct = TRUE, 
                     remove_numbers = TRUE) %>%
                #tokens_select(stopwords('english'),selection='remove') %>%
                tokens_select(dict.regex, selection = 'remove', valuetype = "regex") %>%
                tokens_select(dict.profane, selection = 'remove', valuetype = "fixed")

#see frequency and types of words
dfm <- dfm(total.tokens)
frequency.words <- textstat_frequency(dfm)
n.feat.non.word <- nfeat(dfm)

#remove non-english words
total.tokens <- tokens_select(total.tokens,dict.english, selection = 'keep', valuetype = "fixed")

#see frequency and types of words        
english.dfm <- dfm(total.tokens) 
frequency.eng.words <- textstat_frequency(english.dfm)
n.feat.english.word <- nfeat(english.dfm)


#tokens("New York City is located in the United States.") %>%
#tokens_compound(pattern = phrase(c("New York City", "United States")))

#reduce features but uncapitalizing and word stem
total.tokens <- total.tokens %>%
                #tokens_wordstem() %>%
                tokens_tolower()

num.tokens <- sum(ntoken(total.tokens))

#see frequency and type of words
words.dfm <- dfm(total.tokens)
frequency.word.stem <- textstat_frequency(words.dfm)
n.feat.words <- nfeat(words.dfm)
ndoc.words <- ndoc(words.dfm)

bigram.dfm <- dfm(tokens_ngrams(total.tokens,2))
frequency.bi <- textstat_frequency(bigram.dfm)
n.feat.bigram.word <- nfeat(bigram.dfm)

trigram.dfm <- dfm(tokens_ngrams(total.tokens,3))        
frequency.tri <- textstat_frequency(trigram.dfm)
n.feat.trigram.word <- nfeat(trigram.dfm)

ngram.dfm <- dfm(tokens_ngrams(total.tokens,1:5))
```
  
   Data File name  | # of Lines  |
-------------------|-------------|
en_US.blogs.txt    | 899,289     |
en_US.news.txt     | 1,010,274   |
en_US.twitter.txt  | 2,360,149   |
-------------------|-------------|
  
* Too much data for computer to process so data will need to be randomly sampled - around 20,000 lines per source was taken  

* The number of tokens in the sample was calculated as `r num.tokens`.  
  
* The number of features(kinds of words) in the sample is `r n.feat.words` before the n-grams (combinations of words) were created.  

* A total of `r ndoc.words` lines were in the sample.

* Word count was calculated after tokenizing the data, using regular expressions to remove non-words like website addresses, numbers and profane words

## Exploratory Data Analysis

```{r eval=TRUE,echo=FALSE}
dfm <- ngram.dfm

dfm.trunct <- quanteda::dfm_trim(dfm, min_termfreq = 1)

#get frequency of words
features <- textstat_frequency(dfm) 
features.trunct <- textstat_frequency(dfm.trunct)

#see how distribution of features in bar chart - what number of features
#appear once, twice, etc

distribution.features <- features %>%
            aggregate(by = list(features$frequency), FUN = length) %>%
            rename(frequency.of.word = Group.1) %>%
            rename(number.features = frequency) %>%
            select(frequency.of.word,number.features)
```

### Wordcloud of Top Features
```{r eval = TRUE, echo = FALSE}
textplot_wordcloud(dfm,max_words = 200)
```
  
No stop words have been removed and as a result, stop words dominate the word cloud.  Stop words are very common words in English that dominate the language like "a","the", and "and".  This is expected and stop words will be initially used in the data sample since the next word could very well be a stop word like "a", "the", "and", "he" and "she"."   

### Distribution of Words
```{r eval = TRUE, echo = FALSE, cache=TRUE}
a <- ggplot(distribution.features, aes(x = number.features, y = frequency.of.word)) +
        geom_point() + 
        theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
        coord_cartesian(ylim = c(0,50))

plot(a)  
```
    
This graph shows the number of features with a frequency of 50 or lower.  It shows that the vast majority of features only appear once.  This includes n-grams up to 5-grams so a lot these might be higher order n-grams.  Still it suggests there will need to be a trade-off between a comprehensive vocabulary for the model and a manageable vocabulary size that both the computer and shinyapps.io website can handle.  
  
### Number of Features Needed to Capture All Words
```{r eval = TRUE, echo = FALSE, cache=TRUE}
features.trunct <- features.trunct %>%
        arrange(rank) %>%
        mutate(cusum.words = cumsum(frequency)) %>%
        mutate(feature.counter = 1) %>%
        mutate(cusum.feature = cumsum(feature.counter)) %>%
        mutate(total.words = sum(frequency)) %>%
        mutate(total.words.per = (cusum.words/total.words)*100) %>%
        arrange(-total.words.per)
#row_number())

c <- ggplot(features.trunct,aes(x=total.words.per,y=cusum.feature)) +
        geom_point() +
        scale_x_reverse(name = "Total words(%)")

plot(c)

```
  
This graph further suggests there will need to be a trade-off between coverage of features and the size of the vocabulary.  We will need about 500,000 features to cover 50% of the total words and about 2.6 million to cover 95% of words in the sample.  
  
## Findings

* Initially, I aimed to have a 95% confidence level that the sample is representative of the population - this was calculated as approximately 664 lines each from the news, blogs and tweets corpuses - the sample was increased to around 2000 lines per corpus  

* However, the number of features(different kind of words) increased significantly (a ratio of `r round(5578464/64189,digit=1)`) when around 20,000 lines per corpus was used - this suggests a larger sample is needed
  
## Recommended Actions

* Factors to augment when creating model  
  + size of data sample
  + experiment with excluding stop words to lower vocabulary size
  + experiment with model used in algorithm
  + experiment with refining words with dictionaries
  + potentially remove words with lower frequency to make vocabulary size more manageable

## Feedback Appreciated

Any feedback or thoughts are appreciated.
